##To read
- @inproceedings{arxiv2016/Zhou,

  author    = {Luowei Zhou, Chenliang Xu, Parker Koch, and Jason J. Corso},
  
  title     = {Watch What You Just Said: Image Captioning with Text-Conditional Attention},
  
  booktitle = {arxiv)},
  
  year = {2016}

}

- @inproceedings{arxiv2016/Pedersoli,

  author    = {Marco Pedersoli and Thomas Lucas and Cordelia Schmid and Jakob Verbeek},
  
  title     = {Areas of Attention for Image Captioning},
  
  booktitle = {arxiv},
  
  year = {2016}
  
}

- @inproceedings{aaai2017/Mun,

  author    = {Jonghwan Mun	and Minsu Cho and Bohyung Han},
  
  title     = {Text-guided Attention Model for Image Captioning},
  
  booktitle = {AAAI},
  
  year = {2017}
  
}

Video	Captioning	with	Listwise	Supervision
Yuan	Liu,	Xue	Li	and	Zhongchao	Shi aaai17

Reference	Based	LSTM	for	Image	Captioning
Minghai	Chen,	Guiguang	Ding,	Sicheng	Zhao,	Hui	Chen,	Jungong	Han	and	Qiang	Liu aaai17

Attention	Correctness:	Machine	Perception	vs	Human	Annotations	in	Neural	Image	
Captioning
Chenxi	Liu,	Junhua	Mao,	Fei	Sha	and	Alan	Yuille aaai17

SPICE: Semantic Propositional Image Caption
Evaluation eccv2016

Leveraging Visual Question Answering for Image-Caption Ranking eccv2016

##Attribute + attention based

- @inproceedings{cvpr2016/You,

  author    = {Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo},
  
  title     = {Image captioning with semantic attention},
  
  booktitle = {CVPR)},
  
  year = {2016}
  
}

**keyword**: CNN+RNN, top-down + bottom-up, attribute

**Problem**: defined by the author, top-down method starts from a “gist” of an image and converts it into words, bottom-up one first comes up with words describing various aspects of an image and then combines them. This paper combines the two modelling strategies.

**Analysis**: Figure 2 explains all.

top-down information is captured by the initial state of the RNN. It's not recurrent.

Bottom-up information is represented by the attributes. This information is used to regulate input and output of the RNN. Specifically, In the input attention model, each attribute word is reweighted by a learned parameter, and inputs as a compliment information to the normal input: previous generated word. In the output attention model, each attribute word is also reweighted by another learned parameter, and inputs as a compliment information to the hidden state to generate the next word.  

The attributes are generated by two methods: non-parametric and parametric. The non-parametric one retrieve the nearest neighbors of a test image in the training set and set the words in the caption as the attributes. The parametric one uses a multi-class classifier or FCN learning attributes from local patches.

**Conclusion**: this is an attentional model conditioned on the textual attributes. This is similar to the 5th variation of iclr2017/Yao, which is the best way to input image and attributes.

**What can I do**: the attention is on attributes, which are words derived from the image. This is different from applying attention on image. Can I add image attention into this model? 

**code and time**: lua, don't know time spending

##Attribute based

- @inproceedings{cvpr2015/Fang,

  author    = {Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig},
 
  title     = {From Captions to Visual Concepts and Back},
 
  booktitle = {CVPR},
 
  year = {2015}

}

**keyword**: Attribute, language model

**problem**: 

**analysis**: 1. MIL for detecting words in the image, 2. GeneratingWord Scores for a Test Image, 3. LM for generating M-best sentences, 4. re-ranking the sentences generated by projecting image and sentences into a common space
- @inproceedings{cvpr2016/Wu,

  author    = {Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel},
  
  title     = {What Value Do Explicit High Level Concepts Have in Vision to Language Problems?},
  
  booktitle = {CVPR)},
  
  year = {2016}
  
}

**key words**: CNN+RNN, attribute

**Problem**: whether avoiding the explicit representation of high-level information leads to the success of CNN+RNN framework in Vision to Language problem (image captioning/single word QA/sentence QA).

**Conclusion**: high-level information is critical.

**Analysis**: 

The basic architecture is CNN+RNN with a intermediate layer for predicting the attribute. The attribute is then used as the first input of the RNN. (shown in Fig. 1)

What's Attribute? The objects detected. This paper uses a region-based multi-label classification framework to do this. (shown in Fig. 2)

For different V2L problems, models are different. (shown in Fig. 3). 
 

- @inproceedings{iclr2017/Yao,

  author    = {Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei},
  
  title     = {Boosting  image captioning with attributes},
  
  booktitle = {ICLR (rejected)},
  
  year = {2017}
  
}

**key words**: CNN+RNN, attribute, where to add attribute

**Problem**: where to add Attributes into RNN part for improving the performance

**Analysis**:

The basic architecture is CNN+RNN+Attribute.

What's Attribute? The objects detected. This work uses MIL for detecting as [cvpr2015/Fang].

The models have 5 variations, the differences are where to add Attributes into the RNN

A<sub>1</sub>: same as [cvpr2016/Wu], image infomation is absent, only attribute is used as the first input for the RNN

A<sub>2</sub> --- A<sub>5</sub>, all explained in his Fig. 1

**Conclusion**: **know** where to add Attributes into RNN part for improving the performance. A<sub>5</sub> is the best, where image is the first input, and attribute is added to every timestep of RNN.

##Attention based
- @inproceedings{icml2015/Xu,

  author    = {Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio.},
  
  title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},

  booktitle = {ICML},

  year = {2015}

}

**key words**: CNN+RNN, attention

**Problem**: incorporating attention into the CNN+RNN framework.

**Analysis**: The basic architecture is CNN+RNN.

The CNN feature is extracted from the last convolutional layer of VGG-16. The input of the RNN has the context, which is defined as the image being applied attention mask. Soft attention is a simple weighted sum without technical RL strategy. Furthermore, regularization term which forces the attention to look over the whole image. 
The figure here explains well the model: http://blog.csdn.net/shenxiaolu1984/article/details/51493673 
and (Fig. 3) in pami2016/kun
and Fig. 2(a) in arxiv2016/Lu

**Conclusion**: Attention simulates how human viewing an image. This paper does alignment automatically. Specifically, the region with attention is also aligned well with the word predicted. This is the first paper using the attention for image captioning. 

- @article{pami2016/kun,

    title={Aligning where to see and what to tell: image captioning with region-based attention and scene-specific contexts},
    
    author={Fu, Kun and Jin, Junqi and Cui, Runpeng and Sha, Fei and Zhang, Changshui},

    journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on (TPAMI)},

    year={2016}

}

**key words**: RNN+CNN, attention, high-level semantic scene analysis, 

**Problem**: 1. align visual regions with words using the attention mechanism as in icml2015/Xu. 2. incorporating scene-specific context that captures higher-level semantic information encoded in an image.

**Analysis**: Globally, this model adds scene information (high level conception) into the LSTM as an extra input.

1. select regions and extract CNN feature from them. (Fig. 2)

2. attention model arch is the same as icml2015/Xu. The difference is that the candidate regions in [icml2015/Xu] is grids while this paper uses more fine regions. (Fig. 3)  

3. Scene-specific LSTM. This is an instantiation of the g-LSTM [iccv2015/Jia]. The scene vector is plugged into the LSTM as shown in Fig. 4. The general pipeline of this model is presented in Fig. 1. The scene vector is served as a global context. It is used to biasing the LSTM, so the question: how to caculate it? 
Two steps: unsupervised clustering of captions into “scene” categories and supervised learning of a classifier to predict the scene categories from the visual appearance.

**Conclusion**: the combination of attention and scene vector is useful 

- @inproceedings{arxiv2016/Lu,

  author    = {Jiasen Lu, Caiming Xiong, Devi Parikh and Richard Socher},
  
  title     = {Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning},
  
  booktitle = {arxiv},
  
  year = {2016}
  
}

**state-of-the-art**

**key words**: RNN+CNN, attention, when to pay attention to the image

**Problem**: The decoder likely requires little to no visual information from the image to predict non-visual words such as “the” and “of”. Other words that may seem visual can often be predicted reliably just from the language model e.g., “sign” after “behind a red stop” or “phone” following “talking on a cell”. 

**Analysis**: the model can attend to the image if necessary, otherwise it will just rely on language model. Fig. 1 describes how this system performs. 

Two important novelties:

1. using current state h<sub>t</sub> to calculate attention, which is different from [icml2015/Xu]. See Fig. 2 for understanding the difference. The performance is better but the explanation I do not understand.

2. the sentinel is a latent representation of what the decoder **already** knows. This paper then extracts **the information from the memory cell**. The sentinel gate \beta is calculated as follows. In the view of candidate regions, te sentinel is another region. Replacing the information extracted from the image by the information from the memory cell, the sentinel information is sent to the softmax layer for deciding which region to attend.

**Conclusion**: using memory cell for extracting the previous knowledge as an extra image region for attending is useful. **BUT** the spatial version is indeed worse than the soft-attention version of [icml2015/Xu]. So why not use adaptive attention on the traditional soft-attention model? 



- @inproceedings{arxiv2016/Jiuxiang,

  author    = {Jiuxiang Gu, Gang Wang, Tsuhan Chen},
  
  title     = {Recurrent Highway Networks with Language CNN for Image Captioning},
  
  booktitle = {arxiv},
  
  year = {2016}
  
}

**keyword**: CNN+RNN, text-conditional attention, time-dependent guiding LSTM (td-gLSTM)

**Problem**: 1), when visual evidence is missing or ambiguous, text should help to complement the visual information. 2) using previous generated text to condition the attention on the image. 

**Analysis**: 

different from gLSTM [iccv2015/Jia], the guiding information depends on time. 

The word guides the image feature at each time step according to eq. 5 and eq. 6

**conclusion**: 1)an end-to-end model. 2) Since word is time-dependent, so the guidence is changed to time-dependent. 

**code**: yes, lua

**not clear**: the textual mask on the attributes simplifies the fusion of text and image. It seems that textual feature reweights the iamge feature. However, there seems to be a typo when explaining the eq. 5

- @inproceedings{nips2016/Yang,

  author    = {Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W. Cohen},
  
  title     = {Review Networks for Caption Generation},
  
  booktitle = {NIPS},
  
  year = {2016}
  
}

**keyword**: CNN+RNN, review network, attention, multi-task learning, supervison signal

**Problem**: First, the attention mechanism proceeds in a sequential manner and thus **lacks global modeling abilities**. More specifically, at the generation step t, the decoded token is conditioned on the attention results at
the current time step ~ht, but has no information about future attention results ~ht' with t' > t. For
example, when there are multiple objects in the image, the caption tokens generated at the beginning focuses on the first one or two objects and is unaware of the other objects, which is potentially suboptimal. Second, previous works show that **discriminative supervision (e.g., predicting word occurrences in the caption) is beneficial for generative models [cvpr2015/Fang]**, but it is not clear how to integrate discriminative supervision into the encoder-decoder framework in an end-to-end manner.

**Analysis**:

This generalizes [icml2015/Xu] by applying attention mechanism on the encoder for extracting a compact global information. The decoder part modifies according to the output of the encoder but its less different from [icml2015/Xu].

The reviwer is a LSTM with a fixed length of time step. Two kinds of reviewers are proposed for resp. image input and text input. 
Fig.1 and Fig.2 clearly indicate how they work. 

section 3.6 talks about when reviwer net is the same as [icml2015/Xu]. The most important one is the setting Tr = Tx, which means the [icml2015/Xu] attention network needs a lot of review step. In their work although how to choose Tr is not covered, its much less than Tx. that means the review net can abstract the information. 
**code**: yes, lua


##Guiding

- @inproceedings{iccv2015/Jia,

  author    = {Xu Jia, Efstratios Gavves, Basura Fernando, Tinne Tuytelaars},

  title     = {Guiding the Long-Short Term Memory Model for Image Caption Generation},

  booktitle = {ICCV},

  year = {2015}

}

**keyword**: CNN+RNN, guiding LSTM (g-LSTM)

**Problem**: We notice that sometimes the generated sentence seems to “drift away” or “lose track” of the original image content, generating a description that is common in the dataset, yet only weakly coupled to the input image. We hypothesize this is because the decoding step needs to find a balance between two, sometimes contradicting, forces: on the one hand, the sentence to be generated needs to describe the image content; on the other hand, the generated sentence needs to fit the language model, with more likely word combinations to be preferred. The system then may “lose track” of the original image content if the latter force starts to dominate. From an image caption generation point of view, however, staying close to the image contentmay be considered the most important of the two.

**Analysis**: They propose g-LSTM, LSTM with an extra input containing the **static** (comparing with arxiv2016/Zhou) semantic information.

The author proposed three sementaic info:

1. retrieve text from image. Retrieval-based guidance: generate a sentence, then feed back as guidance.

2. using sementic embedding representation (emb-gLSTM): word is represented by embedding, with lower dimension. Input is the intermediate result of the first method

3. Image as guidance (img-gLSTM): using the whole image as guidance


**conclusion**: the second method is the best. Using image as the guidance is not the best result.  



##Basic arch: CNN+RNN

- @inproceedings{cvpr2015/Vinyals,

  author    = {Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan},

  title     = {Show and Tell: A Neural Image Caption Generator},

  booktitle = {CVPR},

  year = {2015}

}

**keywords**: CNN+RNN

**Problem**: this first paper formally using the idea of CNN-RNN for image captioning. 

**analysis**: it is the basic model for nearly all the deep based image captioning model. Image is first represented by the output of a CNN, then it is fed into the RNN for decoding.

How to do the **beam search** for generating the sentence is well explained in section right before 4.experiment

**code**: tensorflow. In the im2txt repo.

- @inproceedings{cvpr2015/Donahue,

Author = {Jeff Donahue and Lisa Anne Hendricks and Sergio Guadarrama
             and Marcus Rohrbach and Subhashini Venugopalan and Kate Saenko
             and Trevor Darrell},

   Title = {Long-term Recurrent Convolutional Networks
            for Visual Recognition and Description},

   Year  = {2015},

   Booktitle = {CVPR}

}

**keywords**: same as cvpr2015/Vinyals but generalized to video. No comparison with cvpr2015/Vinyals

- @inproceedings{cvpr2015/Karpathy,

  author    = {Andrej Karpathy, Li Fei-Fei},

  title     = {Deep Visual-Semantic Alignments for Generating Image Descriptions},

  booktitle = {CVPR},

  year = {2015}

}

**keywords**: CNN+RNN

**problem**: the author aligns the image patch with the text snippets. Also, they do the image captioning using a standard CNN+RNN. This is very similar to cvpr2015/Vinyals.

**analysis**: see [cvpr2015/Vinyals]

**code**: neuraltalk2, lua, https://github.com/karpathy/neuraltalk2

- @inproceedings{iclr2015/Mao,
  
  author    = {Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille},
  
  title     = {Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},
  
  booktitle = {ICLR},
  
  year = {2015}

}

**keywords**: CNN+RNN

**Difference with cvpr2015/Vinyals**: 1)  two-layer word embedding (no surprise). 2) we do not use the recurrent layer to store the visual information (clarified below)

**Analysis**: the second difference with cvpr2015/Vinyals is explained in the figure 2: image is input into the RNN for every timestep. No explanation why NIC can't use image at every timestep but they can do.

**conclusion**: relatively lower performance wrt NIC

**code**: tf

- @inproceedings{cvpr2015/Chen,

  author    = {X. Chen and C. L. Zitnick.},

  title     = {Mind’s eye: A recurrent visual representation for image caption generation},

  booktitle = {CVPR},

  year = {2015}

}

**keyword**: RNN+CNN (no LSTM but a replacement)

**problem**: jointly maximized the likelihood of the word **and the observed visual features** given the previous words and their visual interpretations (latent variable). (section 3)

**analysis**: visual -> state (RNN) -> word -> latent variable -> reconstructed visual (Fig. 2). This model can do translation from visual to text and also text to visual.

**conclusion**: a new likelyhood objective. 


##Metric optimization

- @inproceedings{cvpr2017/Liu,  

  author    = {Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy},
  
  title     = {Optimization of image description metrics using policy gradient methods},
  
  booktitle = {CVPR (under review)},
  
  year = {2017}
  
}

*Purpose: an optimization method on the metrics of text generation.*

*Value: It's possible to get high score with this methods*

*Analysis: I did not read it through*

*code: No*


## translation

- @inproceedings{emnlp2015/Luong,

  author    = {Minh-Thang Luong, Hieu Pham, Christopher D. Manning},

  title     = {Effective Approaches to Attention-based Neural Machine Translation},

  booktitle = {EMNLP},

  year = {2015}

}

- @inproceedings{tacl2015/Yin,

  author    = {Wenpeng Yin, Hinrich Schütze, Bing Xiang, Bowen Zhou},

  title     = {ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs},

  booktitle = {TACL},

  year = {2015}

}


- @inproceedings{tacl2014/Kiros,

  author    = {Jamie Ryan Kiros, Ruslan Salakhutdinov, Richard Zemel},

  title     = {Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models },

  booktitle = {TACL},

  year = {2014}

}

- @inproceedings{iclr2015/Bahdanau,

  author    = {Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio},

  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},

  booktitle = {ICLR},

  year = {2015}

}

